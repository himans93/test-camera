<!DOCTYPE html>
<html lang="en">
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<style>
  body {
    font-family: Arial, sans-serif;
    padding: 20px;
    text-align: center;
    background: #f7f7f7;
  }
  #cameraInput { display:none; }
  img {
    max-width: 100%;
    border-radius: 12px;
    margin-top: 16px;
    box-shadow: 0 4px 10px rgba(0,0,0,0.2);
  }

  #analysis {
    background: white;
    margin-top: 20px;
    padding: 14px;
    border-radius: 10px;
    text-align: left;
    box-shadow: 0 2px 8px rgba(0,0,0,0.15);
  }
</style>

<!-- TensorFlow Object Detection -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

<!-- GPT-style reasoning (Transformers.js) -->
<script src="https://cdn.jsdelivr.net/npm/@xenova/transformers"></script>

</head>
<body>

<h2>üì∏ AI Camera + Reasoning</h2>

<label style="padding:14px 20px;background:#4CAF50;color:#fff;border-radius:8px;cursor:pointer;" for="cameraInput">
  Open Camera
</label>

<input type="file" id="cameraInput" accept="image/*" capture="environment">

<div id="preview"></div>
<div id="analysis"></div>

<script>
let detectModel = null;
let vlm = null;

// Load object detection
cocoSsd.load().then(m => {
  detectModel = m;
});

// Load GPT-style vision-language model
(async () => {
  vlm = await pipeline(
    "image-to-text",
    "Xenova/qwen-vl-chat",   // lightweight browser AI
    { device: "webgpu" }
  );
})();

document.getElementById("cameraInput").addEventListener("change", async function(e) {
  const file = e.target.files[0];
  if (!file) return;

  const imgURL = URL.createObjectURL(file);
  const img = new Image();
  img.src = imgURL;

  // ALWAYS show image first
  img.onload = async () => {
    document.getElementById("preview").innerHTML = "";
    document.getElementById("preview").appendChild(img);

    document.getElementById("analysis").innerHTML =
      "<b>Analyzing image...</b><br>Please wait.";

    // Wait for models
    if (!detectModel || !vlm) {
      document.getElementById("analysis").innerHTML =
        "Models are still loading‚Ä¶ Please try again in a few seconds.";
      return;
    }

    // -----------------------
    // 1Ô∏è‚É£ Object Detection
    // -----------------------
    const preds = await detectModel.detect(img);
    const objects = preds.map(p => p.class).join(", ") || "No objects detected";

    // -----------------------
    // 2Ô∏è‚É£ GPT-style reasoning
    // -----------------------
    const reasoning = await vlm(img, {
      prompt: "Describe this scene and explain what it might be."
    });

    const text = reasoning[0].generated_text;

    // -----------------------
    // 3Ô∏è‚É£ Show results
    // -----------------------
    document.getElementById("analysis").innerHTML = `
      <b>Detected objects:</b><br>${objects}<br><br>
      <b>AI Reasoning:</b><br>${text}
    `;
  };
});
</script>

</body>
</html>
