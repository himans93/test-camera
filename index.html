<!DOCTYPE html>
<html lang="en">
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<style>
  body {
    font-family: Arial, sans-serif;
    padding: 20px;
    text-align: center;
    background: #f7f7f7;
  }
  #cameraInput { display:none; }
  img { max-width: 100%; border-radius: 12px; margin-top: 16px; }
  #analysis {
    background: white;
    margin-top: 20px;
    padding: 14px;
    border-radius: 10px;
    text-align: left;
    box-shadow: 0 2px 8px rgba(0,0,0,0.15);
  }
</style>

<!-- TensorFlow Object Detection -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

<!-- GPT-style reasoning (Transformers.js + Qwen) -->
<script src="https://cdn.jsdelivr.net/npm/@xenova/transformers"></script>

</head>
<body>

<h2>üì∏ AI Scene Understanding Prototype</h2>

<label style="padding:14px 20px;background:#4CAF50;color:#fff;border-radius:8px;cursor:pointer;" for="cameraInput">
  Open Camera
</label>

<input type="file" id="cameraInput" accept="image/*" capture="environment">

<div id="preview"></div>
<div id="analysis"></div>

<script>
let detectModel = null;
let vlm = null;

// Load object detection model
cocoSsd.load().then(m => detectModel = m);

// Load GPT-style image reasoning model
(async () => {
  vlm = await pipeline(
    "image-to-text",
    "Xenova/qwen-vl-chat",   // small open-source VLM
    { device: "webgpu" }
  );
})();

// When image taken
document.getElementById("cameraInput").addEventListener("change", async function(e) {
  const file = e.target.files[0];
  if (!file) return;

  const imgURL = URL.createObjectURL(file);
  const img = new Image();
  img.src = imgURL;

  img.onload = async () => {
    document.getElementById("preview").innerHTML = "";
    document.getElementById("preview").appendChild(img);

    // Wait until models load
    if (!detectModel) {
      document.getElementById("analysis").textContent =
        "Loading object detection model‚Ä¶";
      return;
    }
    if (!vlm) {
      document.getElementById("analysis").textContent =
        "Loading GPT reasoning model‚Ä¶";
      return;
    }

    // 1Ô∏è‚É£ Object Detection (TF.js)
    const preds = await detectModel.detect(img);
    const things = preds.map(p => p.class).join(", ");

    // 2Ô∏è‚É£ GPT-style reasoning (Transformers.js)
    const explanation = await vlm(img, {
      prompt: "Explain what you see in this photo in simple terms:"
    });

    const text = explanation[0].generated_text;

    // Show reasoning
    document.getElementById("analysis").innerHTML = `
      <b>Detected Objects:</b><br>${things || "None"}<br><br>
      <b>GPT-Style Explanation:</b><br>${text}
    `;
  };
});
</script>

</body>
</html>

